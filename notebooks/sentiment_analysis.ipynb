{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3891acf7",
   "metadata": {},
   "source": [
    "# References:\n",
    "- Research paper on feature engineering for sentiment analysis (Bag of Words, TF-IDF, word embedding, NLP based preprocessing): https://www.sciencedirect.com/science/article/pii/S1877050919306593\n",
    "\n",
    "- Researcb paper on sentiment analysis techniques (SVMs, Logistic Regression, TF-IDF): https://www.irjmets.com/uploadedfiles/paper//issue_10_october_2023/45265/final/fin_irjmets1697386365.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7806c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile('sentiments.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba74aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2a55554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbdb7b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_  \\\n",
       "1599994  AmandaMarie1028   \n",
       "1599995      TheWDBoards   \n",
       "1599996           bpbabe   \n",
       "1599997     tinydiamondz   \n",
       "1599998   RyanTrevMorris   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "1599994  Just woke up. Having no school is the best fee...                                                                   \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...                                                                   \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...                                                                   \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...                                                                   \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...                                                                   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe579a6",
   "metadata": {},
   "source": [
    "This dataset has six features: \n",
    "\n",
    "1. Target: Polarity of the tweet\n",
    "    - 0 = Negative\n",
    "    - 2 = Neutral\n",
    "    - 3 = Positive\n",
    "\n",
    "2. ids: The id of the tweet\n",
    "\n",
    "3. date: The date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "\n",
    "4. flag: The query (lyx). If there is no query, then this value is NO_QUERY\n",
    "\n",
    "5. user: The user the tweeted\n",
    "\n",
    "6. text: the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f1346e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag           user  \\\n",
       "0       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label our features and target variable\n",
    "df.columns = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "# Double check that it worked\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadaf4ea",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7567af56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "ids       0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure there are no null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17409d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove user mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3af5ad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "df['text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ea31fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          is upset that he can't update his facebook by ...\n",
       "1          i dived many times for the ball. managed to sa...\n",
       "2             my whole body feels itchy and like its on fire\n",
       "3          no, it's not behaving at all. i'm mad. why am ...\n",
       "4                                         not the whole crew\n",
       "                                 ...                        \n",
       "1599994    just woke up. having no school is the best fee...\n",
       "1599995    thewdb.com - very cool to hear old walt interv...\n",
       "1599996    are you ready for your mojo makeover? ask me f...\n",
       "1599997    happy 38th birthday to my boo of alll time!!! ...\n",
       "1599998                                                happy\n",
       "Name: text, Length: 1599999, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that it worked\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029fa9e",
   "metadata": {},
   "source": [
    "Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d3820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of sentiment labels:\n",
      "target\n",
      "4    800000\n",
      "0    799999\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the target variable values\n",
    "target_counts = df['target'].value_counts()\n",
    "print(\"Distribution of sentiment labels:\")\n",
    "print(target_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62918b67",
   "metadata": {},
   "source": [
    "Note: The data classes are nearly perfectly evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a68fd56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\terel\\AppData\\Local\\Temp\\ipykernel_15696\\53399156.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.rename(columns={'target': 'sentiment'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Create our training dataframe with the text and labels\n",
    "train_df = df[['target', 'text']]\n",
    "\n",
    "# Rename target to sentiment\n",
    "train_df.rename(columns={'target': 'sentiment'}, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26a0875c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\terel\\AppData\\Local\\Temp\\ipykernel_15696\\2857997488.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['sentiment'] = train_df['sentiment'].map({0: 0, 4: 1})\n"
     ]
    }
   ],
   "source": [
    "# Change sentiment encodings as follows\n",
    "    # 0: negative\n",
    "    # 1: positive\n",
    "train_df['sentiment'] = train_df['sentiment'].map({0: 0, 4: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29ce0a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   sentiment  1599999 non-null  int64 \n",
      " 1   text       1599999 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5c2d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty texts: 3039\n",
      "Very short text: 3619\n"
     ]
    }
   ],
   "source": [
    "# Check for empty strings after cleaning\n",
    "print(f\"Empty texts: {(train_df['text'].str.strip() == '').sum()}\")\n",
    "print(f\"Very short text: {(train_df['text'].str.len() < 3).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de320207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty texts: 0\n",
      "Very short text: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove empty and short strings\n",
    "train_df['text'].str.strip()\n",
    "train_df = train_df[train_df['text'].str.len() > 3]\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Empty texts: {(train_df['text'].str.strip() == '').sum()}\")\n",
    "print(f\"Very short text: {(train_df['text'].str.len() < 3).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4da27b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random positive samples\n",
      "['so we have a new fresh look! supportguy was fixing it during the nite. i think it was worth it'\n",
      " 'hello twitters'\n",
      " \"hee hee, i'm going to go to sleep now and think of all those fun things and smile throughout my sleep love your guts xo\"\n",
      " \"im so happy with life i feel like i haven't lost anything god, fuck yeah!!\"\n",
      " 'sparkpeople has a twitter group . . . you should check that out to find sparkpeeps!']\n",
      "\n",
      "Random negative samples\n",
      "['ahhww thats mean and horrible did the old man see you write that?'\n",
      " 'needs to buy more csi cause he basically seen all of these like 50 times or more.'\n",
      " \"11th june, how shit. i assume you finish tomoreee? jealous how come you aren't coming on friday btw? miss yooou!\"\n",
      " 'sooooo bored. broke a string practicing fml'\n",
      " 'uughieess is taking forevvv my poor lido eye lids r gonna give out haha']\n"
     ]
    }
   ],
   "source": [
    "print(\"Random positive samples\")\n",
    "print(train_df[train_df['sentiment'] == 1]['text'].sample(5).values)\n",
    "\n",
    "print(\"\\nRandom negative samples\")\n",
    "print(train_df[train_df['sentiment'] == 0]['text'].sample(5).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14616fbb",
   "metadata": {},
   "source": [
    "Note: Duplicates may cause data leakage if the same tweet appears in both the training and validation sets. This may lead to an inflated estimate of held-out error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5b4e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates before: 55535\n",
      "Duplicates after: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate tweets (like retweets etc)\n",
    "print(f\"Duplicates before: {train_df.duplicated(subset=['text']).sum()}\")\n",
    "train_df = train_df.drop_duplicates(subset=['text'])\n",
    "print(f\"Duplicates after: {train_df.duplicated(subset=['text']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afd5da12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words overall:\n",
      "[('i', 735077), ('to', 553465), ('the', 511526), ('a', 374782), ('my', 308947), ('and', 293685), ('you', 228314), ('is', 227970), ('for', 210461), ('in', 208056), ('it', 189202), ('of', 181075), ('on', 158373), ('so', 143201), ('have', 141608), ('that', 126691), (\"i'm\", 126276), ('me', 125535), ('but', 123798), ('just', 123103)]\n",
      "\n",
      "Most common positive words\n",
      "[('i', 282435), ('the', 257515), ('to', 246219), ('a', 194226), ('you', 147104), ('and', 144370), ('my', 123030), ('for', 113147), ('is', 104030), ('in', 96629)]\n",
      "\n",
      "Most common negative words\n",
      "[('i', 452642), ('to', 307246), ('the', 254011), ('my', 185917), ('a', 180556), ('and', 149315), ('is', 123940), ('in', 111427), ('it', 100258), ('for', 97314)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_words = ' '.join(train_df['text']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "print('Most common words overall:')\n",
    "print(word_freq.most_common(20))\n",
    "\n",
    "positive_words = ' '.join(train_df[train_df['sentiment']==1]['text']).split()\n",
    "negative_words = ' '.join(train_df[train_df['sentiment']==0]['text']).split()\n",
    "\n",
    "print('\\nMost common positive words')\n",
    "print(Counter(positive_words).most_common(10))\n",
    "\n",
    "print('\\nMost common negative words')\n",
    "print(Counter(negative_words).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49f1b57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        text_length    word_count\n",
      "count  1.539821e+06  1.539821e+06\n",
      "mean   6.703064e+01  1.292963e+01\n",
      "std    3.503266e+01  6.821159e+00\n",
      "min    4.000000e+00  1.000000e+00\n",
      "25%    3.800000e+01  7.000000e+00\n",
      "50%    6.200000e+01  1.200000e+01\n",
      "75%    9.500000e+01  1.800000e+01\n",
      "max    3.600000e+02  6.400000e+01\n",
      "           text_length  word_count\n",
      "sentiment                         \n",
      "0            68.741518   13.420274\n",
      "1            65.294334   12.431695\n"
     ]
    }
   ],
   "source": [
    "# Analyze word length\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "train_df['word_count'] = train_df['text'].str.split().str.len()\n",
    "\n",
    "print(train_df[['text_length', 'word_count']].describe())\n",
    "\n",
    "print(train_df.groupby('sentiment')[['text_length', 'word_count']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ad717",
   "metadata": {},
   "source": [
    "# TF-IDF Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d6b26f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1231856, 10000)\n",
      "Test set shape: (307965, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Split the data\n",
    "X = train_df['text']\n",
    "y = train_df['sentiment']\n",
    "\n",
    "# Split with stratification to maintiain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size = 0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    ")\n",
    "\n",
    "# Create and fit the TF-IDF vectorizer on training data only\n",
    "tfidf = TfidfVectorizer(\n",
    "        min_df=5,           # Exclude really rare words\n",
    "        max_features=10000, # Keep only the 10,000 most common words\n",
    "        ngram_range=(1,2)   # Track words and pairs of words\n",
    ")\n",
    "\n",
    "# Transform the training and test sets\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test) # Don't fit to the test set \n",
    "\n",
    "# Print shape of the transformed data\n",
    "print(f\"Training set shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Test set shape: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4fcc7c",
   "metadata": {},
   "source": [
    "# Baseline Models\n",
    "\n",
    "Establishing baseline performance using:\n",
    "1. Logistic Regression - Simple but effective for text classification\n",
    "2. Linear SVM - Generally performs well on high-dimensional sparse data like TF-IDF vectors\n",
    "3. Naive Bayes - Works well with discrete features and handles sparce data like TF-IDF vectors effictively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb326aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.8017501988862371\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80    155119\n",
      "           1       0.79      0.81      0.80    152846\n",
      "\n",
      "    accuracy                           0.80    307965\n",
      "   macro avg       0.80      0.80      0.80    307965\n",
      "weighted avg       0.80      0.80      0.80    307965\n",
      "\n",
      "\n",
      "SVM Results:\n",
      "Accuracy: 0.8019774974428913\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80    155119\n",
      "           1       0.79      0.82      0.80    152846\n",
      "\n",
      "    accuracy                           0.80    307965\n",
      "   macro avg       0.80      0.80      0.80    307965\n",
      "weighted avg       0.80      0.80      0.80    307965\n",
      "\n",
      "\n",
      "Training Naive Bayes...\n",
      "\n",
      "Naive Bayes Results:\n",
      "Accuracy:  0.7790300845875343\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78    155119\n",
      "           1       0.78      0.77      0.78    152846\n",
      "\n",
      "    accuracy                           0.78    307965\n",
      "   macro avg       0.78      0.78      0.78    307965\n",
      "weighted avg       0.78      0.78      0.78    307965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Initilize models\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "svm_model = LinearSVC(max_iter=1000, random_state=42)\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train and evaluate Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "lr_pred = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, lr_pred))\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, lr_pred))\n",
    "\n",
    "# Train and evaluate SVM\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "svm_pred = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nSVM Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, svm_pred))\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, svm_pred))\n",
    "\n",
    "# Train and evaluate Naive Bayes\n",
    "print(\"\\nTraining Naive Bayes...\")\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "nb_pred = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nNaive Bayes Results:\")\n",
    "print(\"Accuracy: \", accuracy_score(y_test, nb_pred))\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, nb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b50786dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mmost influential features in Logistic Regression:\n",
      "\n",
      "Top positive features:\n",
      "cant wait: 7.5503\n",
      "not bad: 6.8374\n",
      "smile: 5.9716\n",
      "no problem: 5.8096\n",
      "thanks: 5.7104\n",
      "happy: 5.3644\n",
      "can wait: 4.7939\n",
      "congratulations: 4.6870\n",
      "proud: 4.5489\n",
      "smiling: 4.5387\n",
      "\n",
      "Top negative features:\n",
      "sad: -13.6319\n",
      "sadly: -8.7481\n",
      "poor: -8.6485\n",
      "miss: -8.4607\n",
      "unfortunately: -8.1018\n",
      "disappointed: -7.5717\n",
      "died: -7.5531\n",
      "missing: -7.5036\n",
      "not happy: -7.1715\n",
      "sick: -7.1222\n"
     ]
    }
   ],
   "source": [
    "# Compare most important features for Logistic Regression\n",
    "def print_top_features(vectorizer, model, n=10):\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        coef = model.coef_[0]\n",
    "        top_positive = np.argsort(coef)[-n:]\n",
    "        top_negative = np.argsort(coef)[:n]\n",
    "\n",
    "        print(\"\\nTop positive features:\")\n",
    "        for idx in reversed(top_positive):\n",
    "            print(f\"{feature_names[idx]}: {coef[idx]:.4f}\")\n",
    "\n",
    "        print(\"\\nTop negative features:\")\n",
    "        for idx in top_negative:\n",
    "            print(f\"{feature_names[idx]}: {coef[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nMmost influential features in Logistic Regression:\")\n",
    "print_top_features(tfidf, lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a345dd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\terel\\projects\\sentiment-analysis\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 1539821/1539821 [11:02<00:00, 2323.31 examples/s]\n",
      "Map: 100%|██████████| 307965/307965 [01:58<00:00, 2591.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\terel\\projects\\sentiment-analysis\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='827' max='288717' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   827/288717 54:51 < 319:04:01, 0.25 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from transformers import (\n",
    "#     DistilBertTokenizer,\n",
    "#     DistilBertForSequenceClassification,\n",
    "#     TrainingArguments,\n",
    "#     Trainer\n",
    "# )\n",
    "\n",
    "# import torch\n",
    "# from datasets import Dataset\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# import numpy as np\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\n",
    "#     'distilbert-base-uncased', \n",
    "#     num_labels=2\n",
    "# )\n",
    "\n",
    "# # Prepare dataset\n",
    "# def tokenize_data(examples):\n",
    "#     return tokenizer(\n",
    "#         examples['text'],\n",
    "#         padding='max_length',\n",
    "#         truncation=True,\n",
    "#         max_length=128\n",
    "#     )\n",
    "\n",
    "# # Rename 'sentiment' to 'labels'\n",
    "# train_dataset = Dataset.from_pandas(\n",
    "#     train_df[['text', 'sentiment']].rename(columns={'sentiment': 'labels'})\n",
    "# )\n",
    "# test_dataset = Dataset.from_pandas(\n",
    "#     pd.DataFrame({'text': X_test, 'labels': y_test})  # Changed to 'labels'\n",
    "# )\n",
    "\n",
    "# # Tokenize datasets\n",
    "# train_encoded = train_dataset.map(tokenize_data, batched=True)\n",
    "# test_encoded = test_dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "# # Set format to PyTorch tensors\n",
    "# train_encoded.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# test_encoded.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# # Add compute_metrics\n",
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     return {'accuracy': acc}\n",
    "\n",
    "# # Update training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./distilbert_results\",\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     warmup_steps=500,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=500,  \n",
    "#     eval_strategy=\"epoch\",  # Added\n",
    "#     save_strategy=\"epoch\",  # Added\n",
    "# )\n",
    "\n",
    "# # Create Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_encoded,\n",
    "#     eval_dataset=test_encoded,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# print(\"Starting training...\")\n",
    "# trainer.train()\n",
    "\n",
    "# # Evaluate\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"\\nEvaluation Results: {eval_results}\")\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = trainer.predict(test_encoded)\n",
    "# preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# # Print classification report\n",
    "# print(\"\\nDistilBERT Results:\")\n",
    "# print(classification_report(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
